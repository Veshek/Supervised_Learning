{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ML5YoNrZS_Gc"
   },
   "source": [
    "# Lab Notebook 8 - SVM classification in particle physics\n",
    "\n",
    "In this notebook we will learn about applying SVMs to a larger data set from particle physics.\n",
    "\n",
    "_Data by [Sascha Caron](https://www.nikhef.nl/~scaron/). Modified from the Notebook by Viviana Acquaviva (2023). License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "scz89F7WS_Ge"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.svm import SVC, LinearSVC # New algorithm!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV # New! This will be used to explore different hyperparameter choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-V-0pnD9S_Gf"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "rc('text', usetex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGSpyK91S_Gu"
   },
   "source": [
    "## Step 1\n",
    "\n",
    "Read in features and labels from 'ParticleID_features.csv' and 'ParticleID_labels.txt' using pandas. Read in labels from 'ParticleID_labels.txt'. Explore the data set to get an idea of what it looks like (ex: look at first few rows, shape, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Xd7WnozpS_Gu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = pd.read_csv('ParticleID_features.csv')\n",
    "y = pd.read_csv('ParticleID_labels.txt',header=None)\n",
    "data = np.array(y.iloc[:,0])\n",
    "y = pd.DataFrame(data, columns=['collision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "KAl-0b3ES_Gv",
    "outputId": "3b896999-0a88-4101-e869-42ce0c14d3c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   ID      MET    METphi Type_1        P1        P2        P3        P4  \\\n0   0  62803.5 -1.810010      j  137571.0  128444.0 -0.345744 -0.307112   \n1   1  57594.2 -0.509253      j  161529.0   80458.3 -1.318010  1.402050   \n2   2  82313.3  1.686840      b  167130.0  113078.0  0.937258 -2.068680   \n3   3  30610.8  2.617120      j  112267.0   61383.9 -1.211050 -1.457800   \n4   4  45153.1 -2.241350      j  178174.0  100164.0  1.166880 -0.018721   \n\n  Type_2        P5        P6        P7        P8 Type_3        P9      P10  \\\n0      j  174209.0  127932.0  0.826569  2.332000      b   86788.9  84554.9   \n1      j  291490.0   68462.9 -2.126740 -2.582310     e-   44270.1  35139.6   \n2      j  102423.0   54922.3  1.226850  0.646589      j   60768.9  36244.3   \n3      b   40647.8   39472.0 -0.024646 -2.222800      j  201589.0  32978.6   \n4      j   92351.3   69762.1  0.774114  2.568740      j   61625.2  50086.7   \n\n        P11       P12 Type_4       P13      P14      P15       P16 Type_5  \\\n0 -0.180795  2.187970      j  140289.0  76955.8 -1.19933 -1.302800     m+   \n1 -0.706120 -0.371392     e+   72883.9  26902.2 -1.65386 -3.129630    NaN   \n2  1.102890 -1.434480      j   77714.0  27801.5  1.68461  1.389690      j   \n3 -2.496040  1.137810      j   90096.7  26964.5  1.87132  0.817631      j   \n4  0.652572 -3.012800      j  104193.0  31151.0  1.87641  0.865381      j   \n\n        P17      P18       P19       P20 Type_6  P21  P22  P23  P24 Type_7  \\\n0   85230.6  70102.4 -0.645689 -1.659540    NaN  NaN  NaN  NaN  NaN    NaN   \n1       NaN      NaN       NaN       NaN    NaN  NaN  NaN  NaN  NaN    NaN   \n2   26840.0  24469.3 -0.388937 -1.647260    NaN  NaN  NaN  NaN  NaN    NaN   \n3   28235.4  25887.9 -0.411528  2.024290    NaN  NaN  NaN  NaN  NaN    NaN   \n4  746585.0  26219.3  4.041820 -0.874169    NaN  NaN  NaN  NaN  NaN    NaN   \n\n   P25  P26  P27  P28 Type_8  P29  P30  P31  P32 Type_9  P33  P34  P35  P36  \\\n0  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN   \n1  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN   \n2  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN   \n3  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN   \n4  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN   \n\n  Type_10  P37  P38  P39  P40 Type_11  P41  P42  P43  P44 Type_12  P45  P46  \\\n0     NaN  NaN  NaN  NaN  NaN     NaN  NaN  NaN  NaN  NaN     NaN  NaN  NaN   \n1     NaN  NaN  NaN  NaN  NaN     NaN  NaN  NaN  NaN  NaN     NaN  NaN  NaN   \n2     NaN  NaN  NaN  NaN  NaN     NaN  NaN  NaN  NaN  NaN     NaN  NaN  NaN   \n3     NaN  NaN  NaN  NaN  NaN     NaN  NaN  NaN  NaN  NaN     NaN  NaN  NaN   \n4     NaN  NaN  NaN  NaN  NaN     NaN  NaN  NaN  NaN  NaN     NaN  NaN  NaN   \n\n   P47  P48 Type_13  P49  P50  P51  P52  \n0  NaN  NaN     NaN  NaN  NaN  NaN  NaN  \n1  NaN  NaN     NaN  NaN  NaN  NaN  NaN  \n2  NaN  NaN     NaN  NaN  NaN  NaN  NaN  \n3  NaN  NaN     NaN  NaN  NaN  NaN  NaN  \n4  NaN  NaN     NaN  NaN  NaN  NaN  NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>MET</th>\n      <th>METphi</th>\n      <th>Type_1</th>\n      <th>P1</th>\n      <th>P2</th>\n      <th>P3</th>\n      <th>P4</th>\n      <th>Type_2</th>\n      <th>P5</th>\n      <th>P6</th>\n      <th>P7</th>\n      <th>P8</th>\n      <th>Type_3</th>\n      <th>P9</th>\n      <th>P10</th>\n      <th>P11</th>\n      <th>P12</th>\n      <th>Type_4</th>\n      <th>P13</th>\n      <th>P14</th>\n      <th>P15</th>\n      <th>P16</th>\n      <th>Type_5</th>\n      <th>P17</th>\n      <th>P18</th>\n      <th>P19</th>\n      <th>P20</th>\n      <th>Type_6</th>\n      <th>P21</th>\n      <th>P22</th>\n      <th>P23</th>\n      <th>P24</th>\n      <th>Type_7</th>\n      <th>P25</th>\n      <th>P26</th>\n      <th>P27</th>\n      <th>P28</th>\n      <th>Type_8</th>\n      <th>P29</th>\n      <th>P30</th>\n      <th>P31</th>\n      <th>P32</th>\n      <th>Type_9</th>\n      <th>P33</th>\n      <th>P34</th>\n      <th>P35</th>\n      <th>P36</th>\n      <th>Type_10</th>\n      <th>P37</th>\n      <th>P38</th>\n      <th>P39</th>\n      <th>P40</th>\n      <th>Type_11</th>\n      <th>P41</th>\n      <th>P42</th>\n      <th>P43</th>\n      <th>P44</th>\n      <th>Type_12</th>\n      <th>P45</th>\n      <th>P46</th>\n      <th>P47</th>\n      <th>P48</th>\n      <th>Type_13</th>\n      <th>P49</th>\n      <th>P50</th>\n      <th>P51</th>\n      <th>P52</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>62803.5</td>\n      <td>-1.810010</td>\n      <td>j</td>\n      <td>137571.0</td>\n      <td>128444.0</td>\n      <td>-0.345744</td>\n      <td>-0.307112</td>\n      <td>j</td>\n      <td>174209.0</td>\n      <td>127932.0</td>\n      <td>0.826569</td>\n      <td>2.332000</td>\n      <td>b</td>\n      <td>86788.9</td>\n      <td>84554.9</td>\n      <td>-0.180795</td>\n      <td>2.187970</td>\n      <td>j</td>\n      <td>140289.0</td>\n      <td>76955.8</td>\n      <td>-1.19933</td>\n      <td>-1.302800</td>\n      <td>m+</td>\n      <td>85230.6</td>\n      <td>70102.4</td>\n      <td>-0.645689</td>\n      <td>-1.659540</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>57594.2</td>\n      <td>-0.509253</td>\n      <td>j</td>\n      <td>161529.0</td>\n      <td>80458.3</td>\n      <td>-1.318010</td>\n      <td>1.402050</td>\n      <td>j</td>\n      <td>291490.0</td>\n      <td>68462.9</td>\n      <td>-2.126740</td>\n      <td>-2.582310</td>\n      <td>e-</td>\n      <td>44270.1</td>\n      <td>35139.6</td>\n      <td>-0.706120</td>\n      <td>-0.371392</td>\n      <td>e+</td>\n      <td>72883.9</td>\n      <td>26902.2</td>\n      <td>-1.65386</td>\n      <td>-3.129630</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>82313.3</td>\n      <td>1.686840</td>\n      <td>b</td>\n      <td>167130.0</td>\n      <td>113078.0</td>\n      <td>0.937258</td>\n      <td>-2.068680</td>\n      <td>j</td>\n      <td>102423.0</td>\n      <td>54922.3</td>\n      <td>1.226850</td>\n      <td>0.646589</td>\n      <td>j</td>\n      <td>60768.9</td>\n      <td>36244.3</td>\n      <td>1.102890</td>\n      <td>-1.434480</td>\n      <td>j</td>\n      <td>77714.0</td>\n      <td>27801.5</td>\n      <td>1.68461</td>\n      <td>1.389690</td>\n      <td>j</td>\n      <td>26840.0</td>\n      <td>24469.3</td>\n      <td>-0.388937</td>\n      <td>-1.647260</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>30610.8</td>\n      <td>2.617120</td>\n      <td>j</td>\n      <td>112267.0</td>\n      <td>61383.9</td>\n      <td>-1.211050</td>\n      <td>-1.457800</td>\n      <td>b</td>\n      <td>40647.8</td>\n      <td>39472.0</td>\n      <td>-0.024646</td>\n      <td>-2.222800</td>\n      <td>j</td>\n      <td>201589.0</td>\n      <td>32978.6</td>\n      <td>-2.496040</td>\n      <td>1.137810</td>\n      <td>j</td>\n      <td>90096.7</td>\n      <td>26964.5</td>\n      <td>1.87132</td>\n      <td>0.817631</td>\n      <td>j</td>\n      <td>28235.4</td>\n      <td>25887.9</td>\n      <td>-0.411528</td>\n      <td>2.024290</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>45153.1</td>\n      <td>-2.241350</td>\n      <td>j</td>\n      <td>178174.0</td>\n      <td>100164.0</td>\n      <td>1.166880</td>\n      <td>-0.018721</td>\n      <td>j</td>\n      <td>92351.3</td>\n      <td>69762.1</td>\n      <td>0.774114</td>\n      <td>2.568740</td>\n      <td>j</td>\n      <td>61625.2</td>\n      <td>50086.7</td>\n      <td>0.652572</td>\n      <td>-3.012800</td>\n      <td>j</td>\n      <td>104193.0</td>\n      <td>31151.0</td>\n      <td>1.87641</td>\n      <td>0.865381</td>\n      <td>j</td>\n      <td>746585.0</td>\n      <td>26219.3</td>\n      <td>4.041820</td>\n      <td>-0.874169</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "82IFUA9YS_Gv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  collision\n0     ttbar\n1     ttbar\n2     ttbar\n3     ttbar\n4     ttbar",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>collision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ttbar</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ttbar</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ttbar</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ttbar</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ttbar</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EQH55WhS_Gv"
   },
   "source": [
    "## Step 2\n",
    "\n",
    "The labels are in the form 'ttbar' and '4top'. Turn these categorical (string-type) labels into an array of zeros and ones, where 'ttbar'=0 and '4top'=1 using 'LabelEncoder' from sklearn.preprocessing. Call this new array \"target\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lzB-AaSrS_Gv"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y['collision'] = le.fit_transform(np.array(y['collision']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTJcgL79S_Gx"
   },
   "source": [
    "## Step 3\n",
    "\n",
    "Using describe() on features, look at the \"count\" row. Some columns only contain fractions of the total number of data set rows, due to the variable number of products in each collision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "asuEVhroS_Gy",
    "outputId": "178caa3d-71fc-4db0-c289-3156a303d355"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                ID            MET       METphi            P1            P2  \\\ncount  5000.000000    5000.000000  5000.000000  5.000000e+03  5.000000e+03   \nmean   2499.500000   64071.074332    -0.028916  3.301357e+05  1.540486e+05   \nstd    1443.520003   60525.122480     1.819257  3.068202e+05  1.149469e+05   \nmin       0.000000     290.756000    -3.141010  3.857940e+04  2.825400e+04   \n25%    1249.750000   24352.375000    -1.619905  1.369522e+05  8.883690e+04   \n50%    2499.500000   46814.400000    -0.055612  2.263525e+05  1.182015e+05   \n75%    3749.250000   83032.350000     1.537323  4.077158e+05  1.771265e+05   \nmax    4999.000000  692674.000000     3.141130  3.186360e+06  1.276710e+06   \n\n                P3           P4            P5            P6           P7  \\\ncount  5000.000000  5000.000000  4.997000e+03  4.997000e+03  4997.000000   \nmean     -0.039812    -0.003049  2.527799e+05  1.080302e+05    -0.029936   \nstd       1.361762     1.814855  2.638580e+05  8.136261e+04     1.439105   \nmin      -4.110220    -3.140710  1.087540e+04  1.080000e+04    -4.668790   \n25%      -1.035570    -1.574213  1.007510e+05  6.321840e+04    -1.060500   \n50%      -0.038731    -0.009037  1.659740e+05  8.584360e+04    -0.057428   \n75%       0.943598     1.542370  2.999950e+05  1.238700e+05     1.028340   \nmax       4.141410     3.138540  3.587700e+06  1.146330e+06     4.559150   \n\n                P8            P9            P10          P11          P12  \\\ncount  4997.000000  4.950000e+03    4950.000000  4950.000000  4950.000000   \nmean      0.007327  2.117980e+05   74863.343131    -0.025104     0.011845   \nstd       1.828832  2.510361e+05   46309.512365     1.577316     1.802715   \nmin      -3.140530  1.221050e+04   10639.800000    -4.520250    -3.141480   \n25%      -1.602460  7.636905e+04   46549.475000    -1.125620    -1.547418   \n50%       0.015111  1.288565e+05   62498.400000    -0.040648     0.034238   \n75%       1.605210  2.421225e+05   89587.500000     1.066302     1.570887   \nmax       3.139200  2.800410e+06  788338.000000     4.798090     3.139020   \n\n                P13            P14          P15          P16           P17  \\\ncount  4.717000e+03    4717.000000  4717.000000  4717.000000  4.002000e+03   \nmean   1.805997e+05   57289.049481     0.010723     0.045266  1.780366e+05   \nstd    2.383403e+05   32013.857623     1.634072     1.812078  2.577958e+05   \nmin    1.169190e+04   10818.000000    -4.616550    -3.136130  1.110310e+04   \n25%    5.999090e+04   36097.700000    -1.121240    -1.518030  5.278370e+04   \n50%    9.922610e+04   48949.200000    -0.035512     0.060279  9.206885e+04   \n75%    1.914340e+05   68782.100000     1.159480     1.612220  1.850510e+05   \nmax    2.503590e+06  481884.000000     4.730480     3.139660  3.039490e+06   \n\n                 P18          P19          P20           P21           P22  \\\ncount    4002.000000  4002.000000  4002.000000  2.871000e+03    2871.00000   \nmean    48798.018516     0.015167    -0.031312  1.705620e+05   44042.67015   \nstd     26252.978520     1.744489     1.784248  2.381745e+05   23510.65367   \nmin     10287.000000    -4.778980    -3.139040  1.070330e+04   10066.90000   \n25%     30891.650000    -1.198468    -1.550615  5.007050e+04   28453.95000   \n50%     41054.850000     0.054393    -0.079641  8.593460e+04   37378.30000   \n75%     58596.225000     1.225223     1.508260  1.777845e+05   52731.10000   \nmax    331592.000000     4.882960     3.139910  2.791060e+06  452162.00000   \n\n               P23          P24           P25            P26          P27  \\\ncount  2871.000000  2871.000000  1.889000e+03    1889.000000  1889.000000   \nmean     -0.022948     0.014522  1.628825e+05   41151.069666     0.002228   \nstd       1.806611     1.811101  2.269341e+05   20988.953157     1.815312   \nmin      -4.930230    -3.140380  1.197700e+04   11260.200000    -4.758150   \n25%      -1.250050    -1.586675  4.695560e+04   27963.500000    -1.231420   \n50%      -0.046667     0.040528  7.975460e+04   34681.700000     0.025305   \n75%       1.213660     1.587710  1.714090e+05   48486.200000     1.282950   \nmax       4.760630     3.140370  1.981390e+06  293028.000000     4.781060   \n\n               P28           P29            P30          P31          P32  \\\ncount  1889.000000  1.186000e+03    1186.000000  1186.000000  1186.000000   \nmean      0.006738  1.581409e+05   40250.387015     0.072349    -0.035907   \nstd       1.771888  2.118782e+05   26556.025657     1.836492     1.796932   \nmin      -3.135630  1.380860e+04   10973.300000    -4.606330    -3.132610   \n25%      -1.475380  4.535515e+04   27140.550000    -1.243962    -1.626688   \n50%       0.046141  8.315485e+04   33683.550000     0.156083    -0.015617   \n75%       1.479720  1.747630e+05   45464.775000     1.380947     1.541455   \nmax       3.131490  1.932880e+06  524284.000000     4.802970     3.133340   \n\n                P33            P34         P35         P36           P37  \\\ncount  7.290000e+02     729.000000  729.000000  729.000000  4.420000e+02   \nmean   1.596814e+05   40139.289849    0.061654   -0.045868  1.574039e+05   \nstd    2.308620e+05   30074.756789    1.842798    1.788596  2.165489e+05   \nmin    1.119760e+04   10067.900000   -4.814380   -3.136380  1.615530e+04   \n25%    4.387110e+04   26825.000000   -1.226980   -1.513330  4.410735e+04   \n50%    7.894980e+04   33328.000000    0.072709   -0.052590  7.609735e+04   \n75%    1.714670e+05   42325.500000    1.374130    1.394730  1.837982e+05   \nmax    2.111250e+06  341431.000000    4.372340    3.137560  1.529210e+06   \n\n                 P38         P39         P40           P41            P42  \\\ncount     442.000000  442.000000  442.000000  2.610000e+02     261.000000   \nmean    39703.038235    0.118543    0.024249  1.561160e+05   38173.716092   \nstd     30502.312276    1.872084    1.826435  2.319016e+05   29324.658352   \nmin     10183.700000   -4.803880   -3.135910  2.004750e+04   14800.200000   \n25%     26589.250000   -1.223240   -1.422415  4.092160e+04   25298.300000   \n50%     30942.700000    0.035675    0.090282  7.568430e+04   29479.700000   \n75%     39158.225000    1.532125    1.565650  1.634690e+05   36154.100000   \nmax    369567.000000    4.592710    3.138930  2.073960e+06  220722.000000   \n\n              P43         P44           P45            P46         P47  \\\ncount  261.000000  261.000000  1.270000e+02     127.000000  127.000000   \nmean     0.029455    0.026422  1.631051e+05   34876.849606    0.206978   \nstd      1.884750    1.753017  2.248603e+05   20433.767238    1.998859   \nmin     -4.400470   -3.130690  1.780380e+04   12987.900000   -4.447660   \n25%     -1.413650   -1.270700  4.365005e+04   24742.500000   -1.259230   \n50%     -0.088908   -0.041002  8.050910e+04   28262.800000    0.120301   \n75%      1.416310    1.514030  1.578350e+05   35445.700000    1.727295   \nmax      4.790720    3.120760  1.246080e+06  167840.000000    4.691500   \n\n              P48           P49            P50        P51        P52  \ncount  127.000000  5.600000e+01      56.000000  56.000000  56.000000  \nmean    -0.001085  1.456600e+05   36151.183929  -0.000879   0.219260  \nstd      1.949004  1.943657e+05   25861.883410   1.941707   1.910400  \nmin     -3.139820  2.512510e+04   14836.000000  -4.448760  -2.990730  \n25%     -1.817600  4.112588e+04   24974.125000  -1.243362  -1.490900  \n50%     -0.232455  9.553645e+04   27353.550000  -0.121213   0.128103  \n75%      1.712720  1.754910e+05   33817.950000   1.800682   1.984745  \nmax      3.091510  1.177730e+06  155888.000000   4.151320   3.058890  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>MET</th>\n      <th>METphi</th>\n      <th>P1</th>\n      <th>P2</th>\n      <th>P3</th>\n      <th>P4</th>\n      <th>P5</th>\n      <th>P6</th>\n      <th>P7</th>\n      <th>P8</th>\n      <th>P9</th>\n      <th>P10</th>\n      <th>P11</th>\n      <th>P12</th>\n      <th>P13</th>\n      <th>P14</th>\n      <th>P15</th>\n      <th>P16</th>\n      <th>P17</th>\n      <th>P18</th>\n      <th>P19</th>\n      <th>P20</th>\n      <th>P21</th>\n      <th>P22</th>\n      <th>P23</th>\n      <th>P24</th>\n      <th>P25</th>\n      <th>P26</th>\n      <th>P27</th>\n      <th>P28</th>\n      <th>P29</th>\n      <th>P30</th>\n      <th>P31</th>\n      <th>P32</th>\n      <th>P33</th>\n      <th>P34</th>\n      <th>P35</th>\n      <th>P36</th>\n      <th>P37</th>\n      <th>P38</th>\n      <th>P39</th>\n      <th>P40</th>\n      <th>P41</th>\n      <th>P42</th>\n      <th>P43</th>\n      <th>P44</th>\n      <th>P45</th>\n      <th>P46</th>\n      <th>P47</th>\n      <th>P48</th>\n      <th>P49</th>\n      <th>P50</th>\n      <th>P51</th>\n      <th>P52</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5000.000000</td>\n      <td>5000.000000</td>\n      <td>5000.000000</td>\n      <td>5.000000e+03</td>\n      <td>5.000000e+03</td>\n      <td>5000.000000</td>\n      <td>5000.000000</td>\n      <td>4.997000e+03</td>\n      <td>4.997000e+03</td>\n      <td>4997.000000</td>\n      <td>4997.000000</td>\n      <td>4.950000e+03</td>\n      <td>4950.000000</td>\n      <td>4950.000000</td>\n      <td>4950.000000</td>\n      <td>4.717000e+03</td>\n      <td>4717.000000</td>\n      <td>4717.000000</td>\n      <td>4717.000000</td>\n      <td>4.002000e+03</td>\n      <td>4002.000000</td>\n      <td>4002.000000</td>\n      <td>4002.000000</td>\n      <td>2.871000e+03</td>\n      <td>2871.00000</td>\n      <td>2871.000000</td>\n      <td>2871.000000</td>\n      <td>1.889000e+03</td>\n      <td>1889.000000</td>\n      <td>1889.000000</td>\n      <td>1889.000000</td>\n      <td>1.186000e+03</td>\n      <td>1186.000000</td>\n      <td>1186.000000</td>\n      <td>1186.000000</td>\n      <td>7.290000e+02</td>\n      <td>729.000000</td>\n      <td>729.000000</td>\n      <td>729.000000</td>\n      <td>4.420000e+02</td>\n      <td>442.000000</td>\n      <td>442.000000</td>\n      <td>442.000000</td>\n      <td>2.610000e+02</td>\n      <td>261.000000</td>\n      <td>261.000000</td>\n      <td>261.000000</td>\n      <td>1.270000e+02</td>\n      <td>127.000000</td>\n      <td>127.000000</td>\n      <td>127.000000</td>\n      <td>5.600000e+01</td>\n      <td>56.000000</td>\n      <td>56.000000</td>\n      <td>56.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2499.500000</td>\n      <td>64071.074332</td>\n      <td>-0.028916</td>\n      <td>3.301357e+05</td>\n      <td>1.540486e+05</td>\n      <td>-0.039812</td>\n      <td>-0.003049</td>\n      <td>2.527799e+05</td>\n      <td>1.080302e+05</td>\n      <td>-0.029936</td>\n      <td>0.007327</td>\n      <td>2.117980e+05</td>\n      <td>74863.343131</td>\n      <td>-0.025104</td>\n      <td>0.011845</td>\n      <td>1.805997e+05</td>\n      <td>57289.049481</td>\n      <td>0.010723</td>\n      <td>0.045266</td>\n      <td>1.780366e+05</td>\n      <td>48798.018516</td>\n      <td>0.015167</td>\n      <td>-0.031312</td>\n      <td>1.705620e+05</td>\n      <td>44042.67015</td>\n      <td>-0.022948</td>\n      <td>0.014522</td>\n      <td>1.628825e+05</td>\n      <td>41151.069666</td>\n      <td>0.002228</td>\n      <td>0.006738</td>\n      <td>1.581409e+05</td>\n      <td>40250.387015</td>\n      <td>0.072349</td>\n      <td>-0.035907</td>\n      <td>1.596814e+05</td>\n      <td>40139.289849</td>\n      <td>0.061654</td>\n      <td>-0.045868</td>\n      <td>1.574039e+05</td>\n      <td>39703.038235</td>\n      <td>0.118543</td>\n      <td>0.024249</td>\n      <td>1.561160e+05</td>\n      <td>38173.716092</td>\n      <td>0.029455</td>\n      <td>0.026422</td>\n      <td>1.631051e+05</td>\n      <td>34876.849606</td>\n      <td>0.206978</td>\n      <td>-0.001085</td>\n      <td>1.456600e+05</td>\n      <td>36151.183929</td>\n      <td>-0.000879</td>\n      <td>0.219260</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1443.520003</td>\n      <td>60525.122480</td>\n      <td>1.819257</td>\n      <td>3.068202e+05</td>\n      <td>1.149469e+05</td>\n      <td>1.361762</td>\n      <td>1.814855</td>\n      <td>2.638580e+05</td>\n      <td>8.136261e+04</td>\n      <td>1.439105</td>\n      <td>1.828832</td>\n      <td>2.510361e+05</td>\n      <td>46309.512365</td>\n      <td>1.577316</td>\n      <td>1.802715</td>\n      <td>2.383403e+05</td>\n      <td>32013.857623</td>\n      <td>1.634072</td>\n      <td>1.812078</td>\n      <td>2.577958e+05</td>\n      <td>26252.978520</td>\n      <td>1.744489</td>\n      <td>1.784248</td>\n      <td>2.381745e+05</td>\n      <td>23510.65367</td>\n      <td>1.806611</td>\n      <td>1.811101</td>\n      <td>2.269341e+05</td>\n      <td>20988.953157</td>\n      <td>1.815312</td>\n      <td>1.771888</td>\n      <td>2.118782e+05</td>\n      <td>26556.025657</td>\n      <td>1.836492</td>\n      <td>1.796932</td>\n      <td>2.308620e+05</td>\n      <td>30074.756789</td>\n      <td>1.842798</td>\n      <td>1.788596</td>\n      <td>2.165489e+05</td>\n      <td>30502.312276</td>\n      <td>1.872084</td>\n      <td>1.826435</td>\n      <td>2.319016e+05</td>\n      <td>29324.658352</td>\n      <td>1.884750</td>\n      <td>1.753017</td>\n      <td>2.248603e+05</td>\n      <td>20433.767238</td>\n      <td>1.998859</td>\n      <td>1.949004</td>\n      <td>1.943657e+05</td>\n      <td>25861.883410</td>\n      <td>1.941707</td>\n      <td>1.910400</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>290.756000</td>\n      <td>-3.141010</td>\n      <td>3.857940e+04</td>\n      <td>2.825400e+04</td>\n      <td>-4.110220</td>\n      <td>-3.140710</td>\n      <td>1.087540e+04</td>\n      <td>1.080000e+04</td>\n      <td>-4.668790</td>\n      <td>-3.140530</td>\n      <td>1.221050e+04</td>\n      <td>10639.800000</td>\n      <td>-4.520250</td>\n      <td>-3.141480</td>\n      <td>1.169190e+04</td>\n      <td>10818.000000</td>\n      <td>-4.616550</td>\n      <td>-3.136130</td>\n      <td>1.110310e+04</td>\n      <td>10287.000000</td>\n      <td>-4.778980</td>\n      <td>-3.139040</td>\n      <td>1.070330e+04</td>\n      <td>10066.90000</td>\n      <td>-4.930230</td>\n      <td>-3.140380</td>\n      <td>1.197700e+04</td>\n      <td>11260.200000</td>\n      <td>-4.758150</td>\n      <td>-3.135630</td>\n      <td>1.380860e+04</td>\n      <td>10973.300000</td>\n      <td>-4.606330</td>\n      <td>-3.132610</td>\n      <td>1.119760e+04</td>\n      <td>10067.900000</td>\n      <td>-4.814380</td>\n      <td>-3.136380</td>\n      <td>1.615530e+04</td>\n      <td>10183.700000</td>\n      <td>-4.803880</td>\n      <td>-3.135910</td>\n      <td>2.004750e+04</td>\n      <td>14800.200000</td>\n      <td>-4.400470</td>\n      <td>-3.130690</td>\n      <td>1.780380e+04</td>\n      <td>12987.900000</td>\n      <td>-4.447660</td>\n      <td>-3.139820</td>\n      <td>2.512510e+04</td>\n      <td>14836.000000</td>\n      <td>-4.448760</td>\n      <td>-2.990730</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1249.750000</td>\n      <td>24352.375000</td>\n      <td>-1.619905</td>\n      <td>1.369522e+05</td>\n      <td>8.883690e+04</td>\n      <td>-1.035570</td>\n      <td>-1.574213</td>\n      <td>1.007510e+05</td>\n      <td>6.321840e+04</td>\n      <td>-1.060500</td>\n      <td>-1.602460</td>\n      <td>7.636905e+04</td>\n      <td>46549.475000</td>\n      <td>-1.125620</td>\n      <td>-1.547418</td>\n      <td>5.999090e+04</td>\n      <td>36097.700000</td>\n      <td>-1.121240</td>\n      <td>-1.518030</td>\n      <td>5.278370e+04</td>\n      <td>30891.650000</td>\n      <td>-1.198468</td>\n      <td>-1.550615</td>\n      <td>5.007050e+04</td>\n      <td>28453.95000</td>\n      <td>-1.250050</td>\n      <td>-1.586675</td>\n      <td>4.695560e+04</td>\n      <td>27963.500000</td>\n      <td>-1.231420</td>\n      <td>-1.475380</td>\n      <td>4.535515e+04</td>\n      <td>27140.550000</td>\n      <td>-1.243962</td>\n      <td>-1.626688</td>\n      <td>4.387110e+04</td>\n      <td>26825.000000</td>\n      <td>-1.226980</td>\n      <td>-1.513330</td>\n      <td>4.410735e+04</td>\n      <td>26589.250000</td>\n      <td>-1.223240</td>\n      <td>-1.422415</td>\n      <td>4.092160e+04</td>\n      <td>25298.300000</td>\n      <td>-1.413650</td>\n      <td>-1.270700</td>\n      <td>4.365005e+04</td>\n      <td>24742.500000</td>\n      <td>-1.259230</td>\n      <td>-1.817600</td>\n      <td>4.112588e+04</td>\n      <td>24974.125000</td>\n      <td>-1.243362</td>\n      <td>-1.490900</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2499.500000</td>\n      <td>46814.400000</td>\n      <td>-0.055612</td>\n      <td>2.263525e+05</td>\n      <td>1.182015e+05</td>\n      <td>-0.038731</td>\n      <td>-0.009037</td>\n      <td>1.659740e+05</td>\n      <td>8.584360e+04</td>\n      <td>-0.057428</td>\n      <td>0.015111</td>\n      <td>1.288565e+05</td>\n      <td>62498.400000</td>\n      <td>-0.040648</td>\n      <td>0.034238</td>\n      <td>9.922610e+04</td>\n      <td>48949.200000</td>\n      <td>-0.035512</td>\n      <td>0.060279</td>\n      <td>9.206885e+04</td>\n      <td>41054.850000</td>\n      <td>0.054393</td>\n      <td>-0.079641</td>\n      <td>8.593460e+04</td>\n      <td>37378.30000</td>\n      <td>-0.046667</td>\n      <td>0.040528</td>\n      <td>7.975460e+04</td>\n      <td>34681.700000</td>\n      <td>0.025305</td>\n      <td>0.046141</td>\n      <td>8.315485e+04</td>\n      <td>33683.550000</td>\n      <td>0.156083</td>\n      <td>-0.015617</td>\n      <td>7.894980e+04</td>\n      <td>33328.000000</td>\n      <td>0.072709</td>\n      <td>-0.052590</td>\n      <td>7.609735e+04</td>\n      <td>30942.700000</td>\n      <td>0.035675</td>\n      <td>0.090282</td>\n      <td>7.568430e+04</td>\n      <td>29479.700000</td>\n      <td>-0.088908</td>\n      <td>-0.041002</td>\n      <td>8.050910e+04</td>\n      <td>28262.800000</td>\n      <td>0.120301</td>\n      <td>-0.232455</td>\n      <td>9.553645e+04</td>\n      <td>27353.550000</td>\n      <td>-0.121213</td>\n      <td>0.128103</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3749.250000</td>\n      <td>83032.350000</td>\n      <td>1.537323</td>\n      <td>4.077158e+05</td>\n      <td>1.771265e+05</td>\n      <td>0.943598</td>\n      <td>1.542370</td>\n      <td>2.999950e+05</td>\n      <td>1.238700e+05</td>\n      <td>1.028340</td>\n      <td>1.605210</td>\n      <td>2.421225e+05</td>\n      <td>89587.500000</td>\n      <td>1.066302</td>\n      <td>1.570887</td>\n      <td>1.914340e+05</td>\n      <td>68782.100000</td>\n      <td>1.159480</td>\n      <td>1.612220</td>\n      <td>1.850510e+05</td>\n      <td>58596.225000</td>\n      <td>1.225223</td>\n      <td>1.508260</td>\n      <td>1.777845e+05</td>\n      <td>52731.10000</td>\n      <td>1.213660</td>\n      <td>1.587710</td>\n      <td>1.714090e+05</td>\n      <td>48486.200000</td>\n      <td>1.282950</td>\n      <td>1.479720</td>\n      <td>1.747630e+05</td>\n      <td>45464.775000</td>\n      <td>1.380947</td>\n      <td>1.541455</td>\n      <td>1.714670e+05</td>\n      <td>42325.500000</td>\n      <td>1.374130</td>\n      <td>1.394730</td>\n      <td>1.837982e+05</td>\n      <td>39158.225000</td>\n      <td>1.532125</td>\n      <td>1.565650</td>\n      <td>1.634690e+05</td>\n      <td>36154.100000</td>\n      <td>1.416310</td>\n      <td>1.514030</td>\n      <td>1.578350e+05</td>\n      <td>35445.700000</td>\n      <td>1.727295</td>\n      <td>1.712720</td>\n      <td>1.754910e+05</td>\n      <td>33817.950000</td>\n      <td>1.800682</td>\n      <td>1.984745</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4999.000000</td>\n      <td>692674.000000</td>\n      <td>3.141130</td>\n      <td>3.186360e+06</td>\n      <td>1.276710e+06</td>\n      <td>4.141410</td>\n      <td>3.138540</td>\n      <td>3.587700e+06</td>\n      <td>1.146330e+06</td>\n      <td>4.559150</td>\n      <td>3.139200</td>\n      <td>2.800410e+06</td>\n      <td>788338.000000</td>\n      <td>4.798090</td>\n      <td>3.139020</td>\n      <td>2.503590e+06</td>\n      <td>481884.000000</td>\n      <td>4.730480</td>\n      <td>3.139660</td>\n      <td>3.039490e+06</td>\n      <td>331592.000000</td>\n      <td>4.882960</td>\n      <td>3.139910</td>\n      <td>2.791060e+06</td>\n      <td>452162.00000</td>\n      <td>4.760630</td>\n      <td>3.140370</td>\n      <td>1.981390e+06</td>\n      <td>293028.000000</td>\n      <td>4.781060</td>\n      <td>3.131490</td>\n      <td>1.932880e+06</td>\n      <td>524284.000000</td>\n      <td>4.802970</td>\n      <td>3.133340</td>\n      <td>2.111250e+06</td>\n      <td>341431.000000</td>\n      <td>4.372340</td>\n      <td>3.137560</td>\n      <td>1.529210e+06</td>\n      <td>369567.000000</td>\n      <td>4.592710</td>\n      <td>3.138930</td>\n      <td>2.073960e+06</td>\n      <td>220722.000000</td>\n      <td>4.790720</td>\n      <td>3.120760</td>\n      <td>1.246080e+06</td>\n      <td>167840.000000</td>\n      <td>4.691500</td>\n      <td>3.091510</td>\n      <td>1.177730e+06</td>\n      <td>155888.000000</td>\n      <td>4.151320</td>\n      <td>3.058890</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoQXel9US_Gy"
   },
   "source": [
    "Therefore, we will consider a subset of the data, so we have limited imputing/manipulation problems. Define \"features_lim\" as the new limited data set: it only contains the columns \n",
    "\n",
    "'MET', 'METphi', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11',\n",
    "       'P12',  'P13', 'P14', 'P15', and 'P16'\n",
    "       \n",
    "There may still be some columns with NaN values, so replace NaN with 0 for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_L-a_R5S_Gy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** replacing NaN with 0 is the simplest but worst possible choice - imputing a constant value skews the model. One step up would be to input the mean or median for each column. However, because only a limited number of instances have missing data, the choice of imputing strategy doesn't matter too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bGVvOk0S_Gz"
   },
   "source": [
    "Use \"describe()\" to confirm that the count is the same for all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbnrvY0rS_G0",
    "outputId": "87922e80-d595-4dc9-da1b-34927e2ef184"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8EHnF1OS_G0"
   },
   "source": [
    "## Step 4: labels and benchmarking\n",
    "\n",
    "What percentage of the data has the negative label (0) versus the positive label (1)? What is the accuracy of a classifier that puts everything in the negative class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCtTNHcxS_G0",
    "outputId": "aafd29ac-f80d-40eb-8e50-8d4787aa1d3c",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fIqGwitS_G0"
   },
   "source": [
    "For contrast, a random classifier that just assigns a random value according to class distribution has the following accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b1s2iKUS_G0",
    "outputId": "84c52ae9-fa4b-4f68-a481-2dc7d8eed3b0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve69kWKjS_G1"
   },
   "source": [
    "## Step 5: Let's start with a linear model; model = LinearSVC()\n",
    "\n",
    "1. Define a benchmark linear model, \"bmodel\", using LinearSVC(dual=False).\n",
    "\n",
    "2. Use \"StratifiedKFold\" with 5 splits, shuffle set to True, and a random state of 101 to produce a cross-validation object, \"cv\".\n",
    "\n",
    "3. Run \"cross_validate\", where scoring = 'accuracy' and return_train_score=True. This will output the fit time, the score time, the test score, and the train score. Print these.\n",
    "\n",
    "4. Print the mean and standard deviation of the test_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ixh-JsVOS_G1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZpp047LS_G2",
    "outputId": "1f13b55d-6aeb-4bd9-f8f1-ad1a628d9542"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oBiGeXoS_G2"
   },
   "source": [
    "## Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, standardizing/normalizing data using the entire learning set introduces leakage between train and test set (the test set \"knows\" about the mean and standard deviation of the entire data set). Usually this is not a dramatic effect, but the correct procedure is to derive the scaler within each CV fold (i.e. after separating in train and test), only on the train set, and apply the same transformation to the test set. The model then becomes a pipeline.\n",
    "\n",
    "Similar to lab 5-6, set up a pipeline with StandardScaler and LinearSVC(dual=False,C=1000), use it in cross_validate and report the results. For the test and train scores, print the mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SaXQ-JkS_G2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test and train scores of benchmark_lim_piped, print the mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxC_wvRES_G3",
    "outputId": "1adce87f-995d-4f0c-e476-855bb759b1dd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaaIzsRpS_G4"
   },
   "source": [
    "This should show a significant improvement, and the comparison between test and train scores tells us already something about the problem that we have. We can formalize this by looking at the learning curves, which tell us both about gap between train/test scores, AND whether we need more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDn2AoUOS_G4"
   },
   "source": [
    "## Step 7: Learning Curves\n",
    "\n",
    "As in lab 5-6, construct the learning curve (you can recycle code). What does it tell you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORDR8CfuS_HK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2cIdAkWS_G4"
   },
   "source": [
    "Is there anything we can conclude from this graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2SvyIshS_G5"
   },
   "source": [
    "## Step 8: Parameter optimization via cross-validation\n",
    "\n",
    "When we optimize parameters with a grid search, we choose the parameters that give the best test scores. This is different from what would happen with new data - to do this fairly, at no point of the training procedure we are allowed to look at the test labels. Therefore, we would need to do <b> nested cross validation </b> to avoid leakage between the parameter optimization and the cross validation procedure and properly evaluate the generalization error. For now, we are just looking for the best model so \"simple\" CV is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NNFoiueXS_G5",
    "outputId": "01da5aaf-c655-400b-e57b-0ee14a4066c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('standardscaler', StandardScaler()), ('svc', SVC())],\n",
       " 'verbose': False,\n",
       " 'standardscaler': StandardScaler(),\n",
       " 'svc': SVC(),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'svc__C': 1.0,\n",
       " 'svc__break_ties': False,\n",
       " 'svc__cache_size': 200,\n",
       " 'svc__class_weight': None,\n",
       " 'svc__coef0': 0.0,\n",
       " 'svc__decision_function_shape': 'ovr',\n",
       " 'svc__degree': 3,\n",
       " 'svc__gamma': 'scale',\n",
       " 'svc__kernel': 'rbf',\n",
       " 'svc__max_iter': -1,\n",
       " 'svc__probability': False,\n",
       " 'svc__random_state': None,\n",
       " 'svc__shrinking': True,\n",
       " 'svc__tol': 0.001,\n",
       " 'svc__verbose': False}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given:\n",
    "\n",
    "piped_model = make_pipeline(StandardScaler(), SVC()) #now using the general SVC so I can change the kernel\n",
    "piped_model.get_params() #this shows how we can access parameters both for the scaler and the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFPZcVTwS_G5"
   },
   "source": [
    "We can define a dictionary of parameter values to run the optimization. \n",
    "\n",
    "Note that this might take awhile (5-15 minutes); the early estimates output by this cell may be misleading because more complex models (in particular high gamma) take longer.\n",
    "\n",
    "Once you run this cell, the \"model\" object will have attributes \"best_score_\", \"best_params_\" and \"best_estimator_\", which give us access to the optimal estimator (printed out), as well as \"cv_results_\" that can be used to visualize the performance of all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "iaR-DhChS_G5",
    "outputId": "eb8c3d83-200d-489c-dbb8-92aabb77eb43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best params, best score: 0.8962 {'svc__C': 1.0, 'svc__degree': 2, 'svc__gamma': 'scale', 'svc__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Given:\n",
    "\n",
    "#optimizing SVC: THIS IS NOT YET NESTED CV\n",
    "\n",
    "parameters = {'svc__kernel':['poly', 'rbf'], \\\n",
    "              'svc__gamma':['scale', 0.01, 0.1], 'svc__C':[0.1, 1.0, 10.0, 100.0], \\\n",
    "              'svc__degree': [2, 4]}\n",
    "\n",
    "model = GridSearchCV(piped_model, parameters, cv = StratifiedKFold(n_splits=5, shuffle=True), \\\n",
    "                     verbose = 4, n_jobs = -1, return_train_score=True)\n",
    "\n",
    "model.fit(features_lim,target)\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEfppjOGS_G6"
   },
   "source": [
    "## Step 9\n",
    "\n",
    "Next, we visualize the models in a pandas data frame, and rank them according to their test scores.\n",
    "\n",
    "You may find it useful to look at: \n",
    "1. the mean and std of the test scores\n",
    "2. the mean of the train scores (to evaluate if they differ and the significance of the result)\n",
    "3. fitting time (we can pick a faster model instead of the best model if the scores are comparable)!\n",
    "\n",
    "Let \"scores_lim\" be the dataframe containing \"model.cv_results_\". Print the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tY_72jBHS_G7",
    "outputId": "a1ba6846-051c-48b8-f917-97db27f9b393"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in \"scores_lim\", sort the columns 'params','mean_test_score','std_test_score','mean_train_score', and 'mean_fit_time' by descending 'mean_test_score':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build some intuition around the results, I find it helpful to ask: what hyperparameter values are common to all the best-performing models? Here, for example, the rbf kernel seems to be constantly preferred, while the values of C and gamma seem to only affect the scores only mildly. Note also that the Grid Search is insensitive to moot parameters combinations; for example, here the first three models are identical, because the degree of the polynomial kernel does not matter when using an rbf kernel. This is less than ideal, of course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiHw_EU-S_G7"
   },
   "source": [
    "### Final diagnosis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8lVS2-mS_HC"
   },
   "source": [
    "The problem here is high bias, which is not that surprising given that we are using only a subset of features.\n",
    "\n",
    "We can try two things: making up new features which might help, based on what we know about the problem, and using an imputing strategy to include information about the discarded features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prpkV4jgS_HC"
   },
   "source": [
    "### Step 10: Improve the model. \n",
    "\n",
    "In order to include additional features, use .fillna(0) to replace all nan with zero, and .replace('',0) to replace empty string values. Check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_A5nbG0S_HC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaAeaOZ8S_HD"
   },
   "source": [
    "#### Let's start by looking at what kind of particles we have as a product of the collision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "yXFQVWRuS_HD",
    "outputId": "7172b92a-c670-4515-f675-c4bddd2cac34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', 'b', 'e+', 'e-', 'g', 'j', 'm+', 'm-'], dtype='<U2')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array([features['Type_'+str(i)].values for i in range(1,14)]).astype('str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIAKGgOqS_HE"
   },
   "source": [
    "#### Here are the proposed new features (justification can be found in Chapter 4).\n",
    "    \n",
    "    1. The total number of particles produced\n",
    "    2. The total number of b jets\n",
    "    3. The total number of jets\n",
    "    4. The total number of leptons (electrons, positron, mu+, mu-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "CBd2zGtuS_HE"
   },
   "outputs": [],
   "source": [
    "#count number of non-zero types \n",
    "\n",
    "ntot = np.array([-(np.sum(np.array([features['Type_'+str(i)].values[j] == 0 for i in range(1,14)])) - 13) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "R5TG2m0fS_HE"
   },
   "outputs": [],
   "source": [
    "#count number of b jets \n",
    "\n",
    "nbtot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'b' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "D3K5KSm4S_HE"
   },
   "outputs": [],
   "source": [
    "#Actually, let's count all types (jets, photons g, e-, e+, mu-, mu+)\n",
    "\n",
    "njtot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'j' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "rQhhVIvRS_HE"
   },
   "outputs": [],
   "source": [
    "ngtot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'g' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "5CLntzFrS_HE"
   },
   "outputs": [],
   "source": [
    "n_el_tot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'e-' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "upx72Ry0S_HE"
   },
   "outputs": [],
   "source": [
    "n_pos_tot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'e+' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "7yL61wOTS_HF"
   },
   "outputs": [],
   "source": [
    "n_muneg_tot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'm-' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "DzgFacgzS_HF"
   },
   "outputs": [],
   "source": [
    "n_mupos_tot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'm+' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ygcSxcuNS_HF"
   },
   "outputs": [],
   "source": [
    "n_lepton_tot = n_el_tot + n_pos_tot + n_muneg_tot + n_mupos_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAdBDl-US_HF"
   },
   "source": [
    "Add these new features to the dataset and check the result. How many features are there now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woyzSbSsS_HF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIC5oBplS_HF",
    "outputId": "83cdeb20-5f94-4590-882c-a755bc4e110d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QaMinaQS_HF"
   },
   "source": [
    "### Feature engineering: impact of ad-hoc variables\n",
    "\n",
    "Define 'features_lim_2' as the original 'features_lim' plus the new five hand-crafted features. Apply the piped model (StandardScaler plus LinearSVC) to this new dataset via cross_validate. Compute the mean and standard deviation of the test scores. Any improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y55xL6SQS_HF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wHVYyKQS_HF",
    "outputId": "c1d03e50-e195-40ca-a59b-1dd74086d0d2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajGNZOCvS_HF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEly33GaS_HG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJGrC6XXS_HG",
    "outputId": "c8773996-1a76-48a4-9120-6f7d60996bb3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZTxFHyWS_HG",
    "outputId": "403fca03-412e-4e80-cb88-fd42768ce02d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Knowledge-informed feature engineering is often very successful, more than hyperparameter optimization. Machine learning methods are often tooted for their ability to learn relevant representations, but non-deep-learning methods are less capable to do so, and providing informative features is very helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4q395z_S_HG"
   },
   "source": [
    "We can optimize this model as well; it will take a while, just like the previous time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "znVTZHbsS_HG",
    "outputId": "8d32772b-f7b6-4603-fc1c-5f4aabb1e772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best params, best score: 0.9462 {'svc__C': 1.0, 'svc__degree': 2, 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "#optimizing SVC: Takes a few minutes!\n",
    "piped_model = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "parameters = {'svc__kernel':['poly', 'rbf'], \\\n",
    "              'svc__gamma':['scale', 0.01, 0.1], 'svc__C':[0.1, 1.0, 10.0], 'svc__degree': [2, 4, 8]}\n",
    "\n",
    "nmodels = np.product([len(el) for el in parameters.values()])\n",
    "model = GridSearchCV(piped_model, parameters, cv = StratifiedKFold(n_splits=5, shuffle=True), \\\n",
    "                     verbose = 2, n_jobs = -1, return_train_score=True)\n",
    "model.fit(features_lim_2,target)\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcOdtWVUS_HG",
    "outputId": "1c99321f-982e-4e35-82c7-2b7b5f336c40"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-home message: feature engineering often works best if we use subject matter knowledge, and building more features is not necessarily better."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}