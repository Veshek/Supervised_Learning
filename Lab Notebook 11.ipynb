{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "118db2pzZsZj"
   },
   "source": [
    "# Linear regression: regularization and logistic regression\n",
    "\n",
    "Today we will look at two very useful extensions of linear regression algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1818,
     "status": "ok",
     "timestamp": 1684960373934,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "8J5KkycKZsZl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge, Lasso, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict, KFold\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "matplotlib.rcParams.update({'figure.autolayout': False})\n",
    "matplotlib.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Data\n",
    "\n",
    "First let's generate a dataset with more than one feature variable to explore algorithms that can be used to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1684960373934,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "O79k-SpxZsZq"
   },
   "outputs": [],
   "source": [
    "np.random.seed(16) #set seed for reproducibility purposes\n",
    "\n",
    "x1 = np.arange(100)\n",
    "\n",
    "x2 = np.linspace(0,1,100)\n",
    "\n",
    "x3 = np.logspace(2,3,num=100) \n",
    "\n",
    "ypb = 3*x1 + 0.5*x2 + 15*x3 + 3 + 5*(np.random.poisson(3*x1 + 0.5*x2 + 15*x3,100)-(3*x1 + 0.5*x2 + 15*x3)) \n",
    "                                                    #generate some data with scatter following Poisson distribution \n",
    "                                                    #with exp value = y from linear model, centered around 0\n",
    "        \n",
    "xb = np.vstack((x1,x2,x3)).T # this gives us three features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RCGqFvTZsZt"
   },
   "source": [
    "Now we add correlated features with a polynomial transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1684960373935,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "h7UGprgkZsZt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2, include_bias=False)\n",
    "\n",
    "new_xb = poly.fit_transform(xb) # this dives us 9 features total\n",
    "\n",
    "new_xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhUD1YUkZsZv"
   },
   "source": [
    "## Step 2:  Ridge regression\n",
    "\n",
    "Set up Ridge regression, and determine cross-validated scores for different values of alpha that are logarithmically spaced in the rage between $10^{-6}$ and $10^6$, then find the best alpha that optimizes the test score. For this to be meaningful, the data should be standardized, so please set up a pipeline with Ridge() and the StandardScaler() you used earlier. Make a plot of mean test scores versus alpha. Which scale is more appropriate, linear or logarithmic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1684960577414,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "ZEkJZnA0ZsZw",
    "outputId": "92941bb9-cac4-4b0d-ae99-8766153ff54f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IOt3d-jZsZw"
   },
   "source": [
    "There is a built-in routine for this hyperparameter optimization scan called RidgeCV. Try it out, do you get the same answer as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1684960591136,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "lu6BOdJwZsZx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Yd0FYG5ZsZy"
   },
   "source": [
    "The coefficients of the linear model are strongly affected by regularization. Pick one or two of the features and plot the absolute value of the coefficient for the range of alphas used above in the search scan.  Which scale makes most sense for this plot, linear or logarithmic?\n",
    "\n",
    "Remember that the data must be standardized for this comparison to make sense, so either use a pipeline or fit_transform your data first to standard form using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Lasso regression\n",
    "\n",
    "Repeat the items of step 2 for Lasso regression, now for a smaller range of alpha values $10^{-1}$ and $10^4$. Also, to avoid numerical instabilities, in Lasso set the parameters *max_iter = 10000000, tol = 1e-6*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1684960915211,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "t6J-fLUJZsZ2",
    "outputId": "860c1519-8335-453e-851c-cf8be1446468"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Logistic regression\n",
    "\n",
    "Logistic regression can be used as a classifier on categorical data. To check it out, let's revisit the habitable planet data set from Lab 3, *HPLearningSet.csv*.\n",
    "\n",
    "Load the data set and create a feature matix X with the first two features, mass and period. Fit sklearn's *LogisticRegression(random_state=1,penalty=None)*, print the prediction and the accuracy for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to create our own implementation of logistic regression, using gradient descent. The relevant loss function is the negative of eq. (5.25). First, you need to compute analytically the gradient of the loss function wrt. $\\beta$. This replace the MSE loss in your gradient descent code from lab 10. To facilitate the code, it is useful to also define the sigmoid function eq. (5.21) that returns the probabilities that you will need in the gradient. You could then define another function *predict* that takes the probabilities and returns either 0 or 1, depending on whether the probabilities are less than or greater than 0.5.\n",
    "\n",
    "We can set *np.random.seed(1)* to intialize a suitable solution vector. Remember that a column of ones must be added to the feature matrix X just as before to take care of the offset. Try starting with a learning rate of $\\eta=0.0001$. \n",
    "\n",
    "Print the accuracy and predictions of your own logistic regression and compare to sklearn's answer. You might need a quite large number of iterations (up to $10^6$) to reach the same predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pv030ssBZsZ5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
